<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-11-23 Fri 19:19 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>nonautoregressive seq2seq</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="kuan yu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">nonautoregressive seq2seq</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org286e645">1. formally</a></li>
<li><a href="#orgd5d67f0">2. why</a></li>
<li><a href="#org39ddab1">3. how</a>
<ul>
<li><a href="#org1b1df74">3.1. batched decoding</a></li>
<li><a href="#org98d0709">3.2. alternative decoding</a></li>
<li><a href="#orgd322fa7">3.3. alternative input</a></li>
<li><a href="#org3c6872c">3.4. iterative refinement</a></li>
<li><a href="#org164d1e8">3.5. latent models</a></li>
<li><a href="#org0be989f">3.6. summary</a></li>
</ul>
</li>
<li><a href="#orgb1e5988">4. training strategies</a>
<ul>
<li><a href="#orgbaf6861">4.1. weighted loss over sequence</a></li>
<li><a href="#org54c3260">4.2. target dropout</a></li>
<li><a href="#orge0f9787">4.3. word dropout</a></li>
<li><a href="#org5fb7b5a">4.4. iterative refinement</a></li>
</ul>
</li>
<li><a href="#orgb9d95e6">5. modeling strategies</a></li>
</ul>
</div>
</div>

<div id="outline-container-org286e645" class="outline-2">
<h2 id="org286e645"><span class="section-number-2">1</span> formally</h2>
<div class="outline-text-2" id="text-1">
<p>
let \(S\) and \(T\) be vocabularies, or rather spaces of distribution over vocabularies.
</p>

<p>
the notion of a sequence of length \(l : \mathbb{N}\) can be defined in the language of <a href="https://ncatlab.org/nlab/show/dependent+type">dependent type theory</a> as follows.
</p>

\begin{align*}
[l] &= \{ 0 , \ldots , l \}\\
T^{l} &= [l] \to T\\
&= \prod_{n : [l]} T^{n} \to T\\
\end{align*}

<p>
the <a href="https://ncatlab.org/nlab/show/dependent+product+type">dependent product</a> \(\prod_{n : [l]} T^{n} \to T\) is a family of function types from every \(n\)-ary product of \(T\) to \(T\),
namely \(T^{0} \to T\) and \(T^{1} \to T\) and &#x2026; and \(T^{l-1} \to T\), where \(T^{0}\) is the nullary product aka the unit type.
intuitively, if we start from the unit object and construct the sequence with these functions step by step,
it is the same as having a function which constructs the sequence and gives random access to its components.
the dependent product factorization is what gives us autoregressive models.
</p>

<p>
fixing a length \(l : \mathbb{N}\), we can give these equivalent discriptions for a seq2seq model.
</p>

\begin{align*}
       &S^{l} \to T^{l} &&\textrm{straight mapping}\\
=\quad &S^{l} \to [l] \to T &&\textrm{nonautoregressive}\\
=\quad &S^{l} \to \prod_{n : [l]} T^{n} \to T &&\textrm{autoregressive}\\
\end{align*}

<p>
in general, for sequences of variable lengths, we have these seq2seq models.
</p>

\begin{align*}
       &\prod_{m : \mathbb{N}} S^{m} \to \sum_{l : \mathbb{N}} T^{l} &&\textrm{straight mapping}\\
=\quad &\prod_{m : \mathbb{N}} S^{m} \to \sum_{l : \mathbb{N}} [l] \to T &&\textrm{nonautoregressive}\\
=\quad &\prod_{m : \mathbb{N}} S^{m} \to \sum_{l : \mathbb{N}} \prod_{n : [l]} T^{n} \to T &&\textrm{autoregressive}\\
\end{align*}

<p>
the <a href="https://ncatlab.org/nlab/show/dependent+sum">dependent sum</a> \(\sum_{l : \mathbb{N}} T^{l}\) asserts the existence of target sequences of some length.
the prediction of target lengths can be incorporated into the models as follows.
</p>

\begin{align*}
       &\prod_{m : \mathbb{N}} \sum_{f : S^{m} \to \mathbb{N}} \prod_{s : S^{m}} T^{f s} &&\textrm{straight mapping}\\
=\quad &\prod_{m : \mathbb{N}} \sum_{f : S^{m} \to \mathbb{N}} \prod_{s : S^{m}} [f s] \to T &&\textrm{nonautoregressive}\\
=\quad &\prod_{m : \mathbb{N}} \sum_{f : S^{m} \to \mathbb{N}} \prod_{s : S^{m}} \prod_{n : [f s]} T^{n} \to T &&\textrm{autoregressive}\\
\end{align*}

<p>
we can check that the length predicted models are a subset of the general models with the proof
\(\lambda_{g} \lambda_{m} \lambda_{s} \left( g \, m \, 0 \, s, g \, m \, 1 \, s \right) : \left( \prod_{m : \mathbb{N}} \sum_{f : S^{m} \to \mathbb{N}} \prod_{s : S^{m}} T^{f s} \right) \to \left( \prod_{m : \mathbb{N}} S^{m} \to \sum_{l : \mathbb{N}} T^{l} \right)\).
</p>
</div>
</div>

<div id="outline-container-orgd5d67f0" class="outline-2">
<h2 id="orgd5d67f0"><span class="section-number-2">2</span> why</h2>
<div class="outline-text-2" id="text-2">
<p>
autoregressive models are the standard for sequence modeling.
it fits the structure of recurrent networks,
even though it is usually trained with <a href="https://dl.acm.org/citation.cfm?id=1351135">teacher-forcing</a>.
with convolutional and self-attentive networks,
as well as recurrent networks with no hidden-to-hidden connections,
teacher-forcing make the training of all steps parallelizable.
however during inference they all have to run sequentially.
and autoregressive inference involes an exponentially increasing search space.
greedy decoding gives suboptimal results,
and the beam search is commonly used as the remedy,
which is ad hoc and expensive.
autoregressive models are intrinsically slow.
on top of that, teacher-forcing training makes the model less robust during inference,
where it encounters partial sequences unlike the groundtruth ones provided during training.
remedies such as <a href="https://arxiv.org/abs/1506.03099">scheduled sampling</a> can be applied,
however these remedies are also ad hoc and expensive.
</p>

<p>
it has been observed that the partial sequences provided during autoregressive decoding can be defective.
in fact in training variational autoencoders for language modeling,
as suggested by <a href="https://arxiv.org/abs/1511.06349">bowman et al. (2015)</a>,
providing defective partial sequences by randomly dropping words during training
is used to regularize the decoder,
so that it does not simply becomes a language model alone and ignores the encoder.
<a href="https://arxiv.org/abs/1710.10380">tang et al. (2018)</a> trained models for sentence representation learning
with teacher-forcing (perfect partial sequences) and random sampling (defect partial sequences),
and found that they achieve equivalent results.
they further experimented with nonautoregressive convolutional decoders which predict all words simultanously,
and found them to be equivalent to autoregressive ones.
this is however not what people usually find with nonautoregressive models.
usually nonautoregressive perform worse than autoregressive ones.
</p>

<p>
an effective non ad hoc nonautoregressive architecture is still to be discovered.
</p>
</div>
</div>

<div id="outline-container-org39ddab1" class="outline-2">
<h2 id="org39ddab1"><span class="section-number-2">3</span> how</h2>
<div class="outline-text-2" id="text-3">
<p>
most of the research comes from machine translation, unless mentioned otherwise.
</p>
</div>

<div id="outline-container-org1b1df74" class="outline-3">
<h3 id="org1b1df74"><span class="section-number-3">3.1</span> batched decoding</h3>
<div class="outline-text-3" id="text-3-1">
<p>
one remedy is to shorten the autoregressive steps by batching multiple predictions at once.
in tacotron <a href="https://arxiv.org/abs/1703.10135">wang et al. (2017)</a> found that predicting multiple frames at once
in fact helped learning the attention alignment between the decoder and the encoder.
</p>

<p>
<a href="https://arxiv.org/abs/1808.08583">wang et al. (2018)</a> used this approach which they dubbed semi-autoregressive.
the decoder produces a shorter hidden sequence autoregressive,
then the full sequence is produced in parallel from the hidden sequence.
</p>
</div>
</div>

<div id="outline-container-org98d0709" class="outline-3">
<h3 id="org98d0709"><span class="section-number-3">3.2</span> alternative decoding</h3>
<div class="outline-text-3" id="text-3-2">
<p>
<a href="https://arxiv.org/abs/1702.02429">gu et al. (2017)</a>
<a href="https://arxiv.org/abs/1804.07915">chen et al. (2018)</a>
</p>
<ul class="org-ul">
<li>trainable greedy decoding</li>
<li>actor network</li>
</ul>
</div>
</div>

<div id="outline-container-orgd322fa7" class="outline-3">
<h3 id="orgd322fa7"><span class="section-number-3">3.3</span> alternative input</h3>
<div class="outline-text-3" id="text-3-3">
<p>
<a href="https://arxiv.org/abs/1711.02281">gu et al. (2018)</a>
</p>
<ul class="org-ul">
<li>non-autoregressive</li>
<li>fertility feature</li>
<li>self-attention current position mask</li>
</ul>

<p>
<a href="https://arxiv.org/abs/1811.04719">libovick√Ω &amp; helcl (2018)</a>
</p>
<ul class="org-ul">
<li>projected encoder states</li>
<li>ctc loss</li>
</ul>
</div>
</div>

<div id="outline-container-org3c6872c" class="outline-3">
<h3 id="org3c6872c"><span class="section-number-3">3.4</span> iterative refinement</h3>
<div class="outline-text-3" id="text-3-4">
<p>
<a href="https://arxiv.org/abs/1802.06901">lee et al. (2018)</a>
</p>
<ul class="org-ul">
<li>deterministic non-autoregressive sequence modeling</li>
<li>iterative refinement</li>
</ul>
</div>
</div>

<div id="outline-container-org164d1e8" class="outline-3">
<h3 id="org164d1e8"><span class="section-number-3">3.5</span> latent models</h3>
<div class="outline-text-3" id="text-3-5">
<p>
<a href="https://arxiv.org/abs/1711.10433">van den oord et al. (2017)</a>
</p>
<ul class="org-ul">
<li>parallel wavenet</li>
<li>probability density distillation</li>
<li>inverse autoregressive flows <a href="https://arxiv.org/abs/1606.04934">kingma et al. (2016)</a></li>
</ul>

<p>
<a href="https://arxiv.org/abs/1806.04550">schmidt &amp; hofmann (2018)</a>
</p>
<ul class="org-ul">
<li>unconditional word generation</li>
<li>state space model</li>
<li>variational inference</li>
</ul>

<p>
<a href="https://arxiv.org/abs/1805.11063">roy et al. (2018)</a>
</p>
<ul class="org-ul">
<li>vector quentized vae</li>
</ul>

<p>
<a href="https://arxiv.org/abs/1803.03382">kaiser et al. (2018)</a>
</p>
<ul class="org-ul">
<li>autoregressive discrete latent variables</li>
<li>parallel decoding from latent sequence</li>
<li>decomposed vector quantization</li>
</ul>
</div>
</div>

<div id="outline-container-org0be989f" class="outline-3">
<h3 id="org0be989f"><span class="section-number-3">3.6</span> summary</h3>
<div class="outline-text-3" id="text-3-6">
<p>
even though autoregressive, nonautoregressive, and straight mapping are equivalent descriptions,
they are not equivalent in modeling difficulties.
one may put this under intentionality and expressiveness.
consider the fibonacci sequence,
one can predict a member at any position given its two predecessors,
but it is difficult to predict the member unconditionally.
without memoization, the member has to be found recursively.
autoregressive models explicitely models the recursive (<a href="https://ncatlab.org/nlab/show/cocycle">anamorphic</a>) computation.
</p>

<p>
without the autoregressive structure,
the dependencies between the steps have to be factorized and modeled elsewhere.
the decoder must accept some input for every step which faciliates this factorization.
for autoregressive decoders, this input is the partial sequence, and the factorization is the sequential structure.
this input can be defective, but it may have to be different for every sequence.
</p>

<p>
suppose this input can be constant,
it would mean that the target steps are independent given the encoder outputs.
this may as well be true.
even though a source sentence have many target translations,
we only want one of them which is ideally the optimal translation.
however the model fails to learn with position-wise cross-entropy training.
so either we change the training strategy,
or we find some decoder inputs for different sequences.
i file them under training strategies and modeling strategies.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb1e5988" class="outline-2">
<h2 id="orgb1e5988"><span class="section-number-2">4</span> training strategies</h2>
<div class="outline-text-2" id="text-4">
<p>
consider a decoder which accepts only the encoder outputs as the input,
namely a straight mapping model.
</p>

<p>
it may additionally use some other trained information to query the encoder outputs.
the trained information is only dependent on the decoding position,
namely a position embedding.
</p>
</div>

<div id="outline-container-orgbaf6861" class="outline-3">
<h3 id="orgbaf6861"><span class="section-number-3">4.1</span> weighted loss over sequence</h3>
<div class="outline-text-3" id="text-4-1">
<p>
the model is capable of contructing the first few steps just fine.
the softmax probabilities for the first few steps (as well as the final padding positions) are high (over 60%),
but it degrades under 20% very soon and outputs only whitespaces.
we also observe this in an autoregressive model where instead of feeding discrete inputs,
we feed the softmax probabilities to multiple with the embedding matrix.
the accumulation of uncertainties makes the model less and less confident until it cannot make a decision.
why this happens for a nonautoregressive model with no sequential structure?
</p>

<p>
consider applying linearly increasing weights on the loss over a sequence of length \(t\):
\([1 , 2 , 3 , \ldots , t, 1 , \ldots , 1]\).
</p>

<p>
so far does not seem to work and even hinders learning.
and it hinders less when i softened the weights by sqrt.
</p>
</div>
</div>

<div id="outline-container-org54c3260" class="outline-3">
<h3 id="org54c3260"><span class="section-number-3">4.2</span> target dropout</h3>
<div class="outline-text-3" id="text-4-2">
<p>
consider a denoising decoder which takes a target sequence and predict the same sequence,
with no causal mask but a very high dropout rate.
similar idea as feeding defect sequences.
increase the dropout rate over time, from 50% to 100% by the logistic curve,
specifically \(\operatorname{sigmoid}\left( s / 1e5 \right)\) where \(s\) is the training step.
</p>

<p>
we add the position embedding to the embedded target sequence.
after the dropout rate reaches almost 100%,
we can simply omit the target sequence,
if the model still works.
</p>

<p>
i used a custom dropout where the values are not scaled up,
since when the dropout rate reaches 100%,
that scale will become too high and cause numeric problems.
</p>

<p>
so far the model seems happy.
</p>
</div>
</div>

<div id="outline-container-orge0f9787" class="outline-3">
<h3 id="orge0f9787"><span class="section-number-3">4.3</span> word dropout</h3>
<div class="outline-text-3" id="text-4-3">
<p>
similar to target dropout, but drop entire words.
always drop the current word.
</p>

<p>
todo
</p>

<p>
two methods:
</p>
<ol class="org-ol">
<li>word embedding | position embedding + sinusoidal encoding, randomly swap out word vectors with position vectors</li>
<li>word embedding + position embedding, randomly mask out word vectors</li>
</ol>
</div>
</div>

<div id="outline-container-org5fb7b5a" class="outline-3">
<h3 id="org5fb7b5a"><span class="section-number-3">4.4</span> iterative refinement</h3>
<div class="outline-text-3" id="text-4-4">
<p>
train a logit vector over the target vocabulary for every position.
this replaces the position embedding when softmaxed and mutiplied with the target embedding.
the logit inputs are a crude approximation to the target logits.
from this, the decoder predicts better logits.
take the better logits again as inputs.
iterate this process 4 times with the same decoder parameters.
we take the final logits as outputs,
but compute loss for all 4 intermediate logits, and backprop together.
</p>

<p>
i used only one decoder layer instead of two,
but it still made training 3 times slower.
so far this does not seem to do good.
it got to the same loss and accuracy as before.
the output translations seem more erratic.
</p>

<p>
todo this should be a case of variational inference, find out more
</p>
</div>
</div>
</div>

<div id="outline-container-orgb9d95e6" class="outline-2">
<h2 id="orgb9d95e6"><span class="section-number-2">5</span> modeling strategies</h2>
<div class="outline-text-2" id="text-5">
<p>
todo cf latent models
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: kuan yu</p>
<p class="date">Created: 2018-11-23 Fri 19:19</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
