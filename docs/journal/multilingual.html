<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-01-13 Sun 13:15 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>multilingual</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="kuan yu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">multilingual</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5ef6f3e">1. background</a></li>
<li><a href="#org3375556">2. motivation</a></li>
<li><a href="#org9ace381">3. idea</a></li>
<li><a href="#orgaf5892d">4. data</a></li>
<li><a href="#orge83a903">5. bleu</a></li>
<li><a href="#orga7445de">6. maybe todo</a></li>
</ul>
</div>
</div>

<div id="outline-container-org5ef6f3e" class="outline-2">
<h2 id="org5ef6f3e"><span class="section-number-2">1</span> background</h2>
<div class="outline-text-2" id="text-1">
<p>
a one-way bilingual machine translation model translates from the
source language to the target language.  with the commonly adopted
encoder-decoder structure, the encoder learns to extract information
from the source language, and the decoder is a language model for the
target language conditioned on the encoder outputs.  a naive way to
apply such models for multilingual translation is to train one model
for each source-target mapping.  in order to translate between \(n\)
languages, \(n \times (n-1)\) such models must be trained.  the
complexity is therefore \(O(n^{2})\).
</p>

<p>
<a href="https://arxiv.org/pdf/1601.01073.pdf">firat et al. 2016</a> proposed to share the attention mechanism which
connects the encoder and decoder across all models.  as a result, when
a new language is added, only a new encoder and a new decoder must be
trained in order to connect with the encoders and decoder for other
languages.  the complexity is \(O(n)\).  this also improved the
performance over one-way translation models, especially with
low-resource languages.
</p>

<p>
the newly added language only has to be trained using a parallel
corpus with another existing language.  the system is able to
translate between the new language and any other existing languages,
without ever training on a paired instance.  this is commonly dubbed
zero-shot translation.
</p>

<p>
<a href="https://arxiv.org/pdf/1611.04558.pdf">johnson et al. 2017</a> explained the zero-shot translation system from
google, which adopts a even more aggressive parameter sharing method.
all parameters are shared, including the embeddings for vocabularies.
in order for the decoder to determine which language to generate, a
special <code>bos</code> symbol is added to the shared vocabulary for each
language.  inspection on the sentence embedding space suggests that
the models learns an interlingua representation.
</p>

<p>
it's not clear to me how they handled the problem regarding the size
of the vocabulary.  to model an open vocabulary, they used wordpiece,
so that rare words can be reconstructed from character ngrams.
however even the smallest subset of ngrams, namely single characters,
has over 1 million members, consider the number of unicode code
points.  performing the input lookup is not a problem.  the embedding
table can be split and stored distributedly across multiple machines.
however for the output logit computation, this is obviously not
feasible even with google's computation power.  i suspect they used
sampled softmax.
</p>

<p>
<a href="https://arxiv.org/pdf/1611.04798.pdf">ha et al. 2016</a> proposed to share the encoder and decoder but not the
vocabulary.  the words are further identified with their language
origin, and all words are trained together.  in another word, the
vocabulary in their system is the disjoint union of vocabularies for
all languages.
</p>

<p>
<a href="https://arxiv.org/pdf/1806.06957.pdf">lakew et al. 2018</a> investigated the performace of rnn-based
multilingual translation system and the transformer.  their parameter
sharing approach is most similar to <a href="https://arxiv.org/pdf/1611.04558.pdf">johnson et al. 2017</a>.  they found
that the transformer delivers the best performing models, and some
(maybe surprising) findings:
</p>
<ul class="org-ul">
<li>multilingual models consistently outperform bilingual models;</li>
<li>relatedness of the source and target languages does not play a
crucial role when comparing zero-shot and bilingual models.</li>
</ul>

<p>
<a href="https://arxiv.org/pdf/1809.00252.pdf">sachan et al. 2018</a> investigated in details the optimal parameter
sharing method for a multilingual transformer.  their conclusion is
that it's best to share the embedding, everything in the encoder, but
only the key and query transformations in the decoder.
</p>
</div>
</div>

<div id="outline-container-org3375556" class="outline-2">
<h2 id="org3375556"><span class="section-number-2">2</span> motivation</h2>
<div class="outline-text-2" id="text-2">
<p>
a multilingual translation system with shared parameters must be
trained with batches involving all languges.  usually it's done by
sampling from the disjoint union of all parallel corpora.  even in
<a href="https://arxiv.org/pdf/1601.01073.pdf">firat et al. 2016</a> where only the attention mechanism is shared, all
languages are trained simultaneously.
</p>

<p>
furthermore, most systems with aggressive parameter sharing share the
embedding.  this requires us to fix the vocabularies, leaving no room
for a new language.  on top of that, this violates the <a href="https://ncatlab.org/nlab/show/Yoneda+lemma">foundamental
theory behind representation learning</a> that structure/meaning is the
ways an object is related to every other object in the same
space/language.  consider the word "handy" in german and english.
even in <a href="https://arxiv.org/pdf/1611.04798.pdf">ha et al. 2016</a> where words are additionally identified by
their language, the vocabularies are still updated together in the
logit layer.
</p>

<p>
consider the situation where an unexpected language is encountered,
possibly even a pair of unexpected languages, for example when an
alien civilization sends us a parallel corpus to initialize
communication.  these models will be difficult to adapt.
</p>

<p>
i would like to investigate the possibility for a multilingual
translation system with maximum parameter sharing, which can be
adapted to new languages without training with all supported
langauges, and still able to perform zero-shot translation.
</p>

<p>
the design is a transformer with embeddings parametrized by the
languages.  as usual, the input embedding and output embedding (the
logit layer) are shared (transposed and scaled), but a different
embedding matrix is used for each language.  the majority of the
parameters (the encoder and the decoder) are trained in a pretraining
stage with multiple languages simultaneously, which encourages it to
learn a general encoding-decoding mechanism.  since the only thing
that allows to model to differentiate those languages are the
different embedding matrices, it must try to encode all
language-specific information in the embeddings alone.  and when we
wish to adapt the model to a new language, we simply add a new
embedding matrix and update the parameters there to accomodate it into
the system.
</p>
</div>
</div>

<div id="outline-container-org9ace381" class="outline-2">
<h2 id="org9ace381"><span class="section-number-2">3</span> idea</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>5 germanic languages : en nl de da sv</li>
<li>parallel corpora between en and the other 4</li>
</ul>

<p>
training
</p>

<ul class="org-ul">
<li>model 0: between nl da</li>
<li>model 1: between en de sv</li>
<li>model 2: between nl da, from model 1</li>
<li>model 3: between nl da, from model 1, train embedding only</li>
</ul>

<p>
evaluation
</p>

<ul class="org-ul">
<li>bleu scores between nl da          ; for model 0</li>
<li>bleu scores between en de sv       ; for model 1</li>
<li>bleu scores between en nl de da sv ; for model 2 3</li>
</ul>

<p>
interpretation
</p>

<ul class="org-ul">
<li>m0 is the usual one-way translation baseline</li>
<li>m1 is the pretrained model, whose encoding-decoding mechanism is
expected to be universal</li>
<li>m2 investigates whether
<ul class="org-ul">
<li>the model m1 can be updated with new languages without forgetting the old ones</li>
<li>the model m0 can benefit from pretraining on related languages</li>
</ul></li>
<li>m3 is the main interest of investigation, specifically
<ul class="org-ul">
<li>is it possible for a new pair of languages to adapt to the
encoding-decoding mechanism of m1, with only the freedom of
updating the embeddings from random initialization</li>
<li>is it possible to perform zero-shot translation this way</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgaf5892d" class="outline-2">
<h2 id="orgaf5892d"><span class="section-number-2">4</span> data</h2>
<div class="outline-text-2" id="text-4">
<p>
number of non-empty instances in each corpus
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">nl</td>
<td class="org-right">1978745</td>
</tr>

<tr>
<td class="org-left">de</td>
<td class="org-right">1908920</td>
</tr>

<tr>
<td class="org-left">da</td>
<td class="org-right">1949685</td>
</tr>

<tr>
<td class="org-left">sv</td>
<td class="org-right">1848423</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>partition all sentences in 5 corpora into equivalence classes</li>
<li>take the classes which cover all languages (1390396) uniquely (1381977)</li>

<li>sentencepiece unigram vocabulary model, one for each language</li>
<li>take instances with all sentences within 64 pieces (1259953)</li>
<li>randomly split 4096 instances for evaluation and 1024 for validation</li>
<li>1254833 training instances</li>
</ul>
</div>
</div>

<div id="outline-container-orge83a903" class="outline-2">
<h2 id="orge83a903"><span class="section-number-2">5</span> bleu</h2>
<div class="outline-text-2" id="text-5">
<p>
evaluated with <code>sacrebleu -tok intl</code>
</p>

<p>
after training each model for ~36 epochs, with batch size 300
</p>
<ul class="org-ul">
<li>300k steps for m0 m2 m3</li>
<li>900k steps for m1</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">tgt</th>
<th scope="col" class="org-left">src</th>
<th scope="col" class="org-right">m0</th>
<th scope="col" class="org-right">m1</th>
<th scope="col" class="org-right">m2</th>
<th scope="col" class="org-right">m3</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">nl</td>
<td class="org-left">en</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.2</td>
<td class="org-right">23.3</td>
</tr>

<tr>
<td class="org-left">en</td>
<td class="org-left">nl</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">30.7</td>
</tr>

<tr>
<td class="org-left">de</td>
<td class="org-left">en</td>
<td class="org-right">0.0</td>
<td class="org-right">29.0</td>
<td class="org-right">0.0</td>
<td class="org-right">29.0</td>
</tr>

<tr>
<td class="org-left">en</td>
<td class="org-left">de</td>
<td class="org-right">0.0</td>
<td class="org-right">35.9</td>
<td class="org-right">0.0</td>
<td class="org-right">35.9</td>
</tr>

<tr>
<td class="org-left">da</td>
<td class="org-left">en</td>
<td class="org-right">0.1</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">31.5</td>
</tr>

<tr>
<td class="org-left">en</td>
<td class="org-left">da</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">35.4</td>
</tr>

<tr>
<td class="org-left">sv</td>
<td class="org-left">en</td>
<td class="org-right">0.0</td>
<td class="org-right">36.3</td>
<td class="org-right">0.0</td>
<td class="org-right">36.3</td>
</tr>

<tr>
<td class="org-left">en</td>
<td class="org-left">sv</td>
<td class="org-right">0.0</td>
<td class="org-right">40.1</td>
<td class="org-right">0.0</td>
<td class="org-right">40.1</td>
</tr>

<tr>
<td class="org-left">de</td>
<td class="org-left">nl</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">22.0</td>
</tr>

<tr>
<td class="org-left">nl</td>
<td class="org-left">de</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">22.1</td>
</tr>

<tr>
<td class="org-left">da</td>
<td class="org-left">nl</td>
<td class="org-right">29.8</td>
<td class="org-right">0.0</td>
<td class="org-right">29.4</td>
<td class="org-right">26.3</td>
</tr>

<tr>
<td class="org-left">nl</td>
<td class="org-left">da</td>
<td class="org-right">28.1</td>
<td class="org-right">0.0</td>
<td class="org-right">27.7</td>
<td class="org-right">23.5</td>
</tr>

<tr>
<td class="org-left">sv</td>
<td class="org-left">nl</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">23.7</td>
</tr>

<tr>
<td class="org-left">nl</td>
<td class="org-left">sv</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">21.5</td>
</tr>

<tr>
<td class="org-left">da</td>
<td class="org-left">de</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.2</td>
<td class="org-right">26.9</td>
</tr>

<tr>
<td class="org-left">de</td>
<td class="org-left">da</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">23.6</td>
</tr>

<tr>
<td class="org-left">sv</td>
<td class="org-left">de</td>
<td class="org-right">0.0</td>
<td class="org-right">27.9</td>
<td class="org-right">0.0</td>
<td class="org-right">27.9</td>
</tr>

<tr>
<td class="org-left">de</td>
<td class="org-left">sv</td>
<td class="org-right">0.0</td>
<td class="org-right">25.9</td>
<td class="org-right">0.0</td>
<td class="org-right">25.9</td>
</tr>

<tr>
<td class="org-left">sv</td>
<td class="org-left">da</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">0.0</td>
<td class="org-right">29.0</td>
</tr>

<tr>
<td class="org-left">da</td>
<td class="org-left">sv</td>
<td class="org-right">0.1</td>
<td class="org-right">0.0</td>
<td class="org-right">0.1</td>
<td class="org-right">29.8</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>the ideas regarding m3 are definitely possible, albeit not optimal</li>
<li>the model is not capable of retaining it's old knowledge when
trained on a new language pair, if we allow all parameters to be
updated (m2 vs m1)</li>
<li>if we only allow the new embeddings to be updated, obviously the
performace of the old pairs won't be affected (m3 vs m1); however
the model does not learn as well (3&#x2013;4 bleu lower, m3 vs m2)</li>
<li>surprisingly, pretraining the model even on related languages
degrades the performace (m2 vs m0)</li>
</ul>
</div>
</div>

<div id="outline-container-orga7445de" class="outline-2">
<h2 id="orga7445de"><span class="section-number-2">6</span> maybe todo</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>alignining all corpora is not necessary, i did that to simply the
training schedule and evaluation; consider taking english as the
common language, and introduce only one new language instead of a
pair of new languages; having one language familar to the model in
the new training stage may ease the update of the new embedding from
randomly initialized values to cooperate better with the trained
embedding spaces of the other languages</li>
<li>since the 3 languages used for pretraining are related (en de sv),
the pretrained model may not be general enough to adapt to new
languages, which is to say that the parameters in the encoder and
decoder are still language specific; considering pretraining with
more diverse languages</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: kuan yu</p>
<p class="date">Created: 2019-01-13 Sun 13:15</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
