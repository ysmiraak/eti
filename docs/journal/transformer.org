#+TITLE: transformer
#+AUTHOR: kuan yu

* preprocessing

[[https://github.com/google/sentencepiece][sentencepiece]] supports *bpe* ([[https://www.aclweb.org/anthology/P16-1162][sennrich et al.]]) and *unigram* ([[https://arxiv.org/abs/1804.10959][kudo]]) segmentations.
the latter supports non-deterministically sampled segmentations.

with the same vocabulary size, *unigram* produces shorter sequences than *bpe*, and performs slightly better.

training with sampled segmentations takes much longer to reach the same results.
eventually it should produce a better model, according to kudo.
however it does require tuning two hyperparameters, the smoothing rate and the sampling size.
for now i use only deterministic segmentations for training.

bleu scores with two layered transformer (=smsm-samsam= architecture).

| steps   | 100k | 200k |
|---------+------+------|
| sampled | 21.7 | 23.8 |
| unigram | 32.7 | 33.5 |
| bpe     | 32.8 | 33.0 |

* initialization

by default tensorflow uses *glorot* initialization ([[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf][glorot & bengio]]),
and it's common to use *he* initialization ([[https://arxiv.org/abs/1502.01852][he et al.]]) on layers with relu activation.
both are special cases of variance scaling.
with uniform initialization,
*glorot* sets the bound by \(\sqrt{3/n}\) and *he* by \(\sqrt{6/n}\),
where \(n\) is either \(\texttt{fan_in}\) (the input dimension),
\(\texttt{fan_out}\) (the output dimension),
or \(\texttt{fan_avg}\) (the average of the two).
the doubled scaling factor in *he* can be understood from the sparse nature of relu.

using \(\texttt{fan_in}\) preserves the variance during the forward propogation,
and \(\texttt{fan_out}\) preserves the variance during the backward propogation.
either one is sufficient condition for the model to converge.
\(\texttt{fan_avg}\) makes a trade-off between the two,
which i cannot make sense of.
usually *glorot* is used with \(\texttt{fan_avg}\) while *he* with \(\texttt{fan_in}\),
due to the suggestions of the authors.
however \(\texttt{fan_out}\) makes more sense.
consider a linear transformation \(\mathbb{R}^{p+q} \to \mathbb{R}^{r}\),
it can be factored into \(\mathbb{R}^{p} \to \mathbb{R}^{r}\) and \(\mathbb{R}^{q} \to \mathbb{R}^{r}\)
followed by vector addition.
only with \(\texttt{fan_out}\) that these two equivalent designed would be initialized similarly.

layer normalization makes initialization less important.
these variations result in little difference.
however input-output embedding sharing ([[https://arxiv.org/abs/1608.05859][press & wolf]]) and sinusoidal position encoding used in the transformer
introduces some complications.
since the sinusoidal encoding produces values in range \((-1,1)\),
the values in the input embedding cannot be too small.
however if they are on the same scale as the sinusoids,
the values in the ouput embedding would be too large for a logit layer,
since softmax activation will exponentiate those logits.
in the transformer, the shared matrix is initialized as usual with *glorot*,
but scaled up by \(\sqrt{d}\) when used as the input embedding,
where \(d\) is the model dimension.
this scaling seems reasonable by the same reasoning for scaled dot-product attention.
however when we consider how it interplays with the initialization scheme,
it is in fact an ad hoc remedy and can be too aggressive in extreme cases.
consider a character-level model with \(256\) characters and \(512\) model dimension,
the output embedding would be initialized with bound \(0.0884 \approx \sqrt{3 / ((512 + 256) / 2)}\),
which after up-scaling by \(\sqrt{512} \approx 22.63\)
results in an input embedding uniformly initialized with bound \(2.0\).

when the weights are initialized by \(\texttt{fan_out}\),
the scale of the input and output embeddings can be gracefully translated with
\(\sqrt{\texttt{fan_out}/\texttt{fan_in}}\)
since \(\sqrt{3/\texttt{fan_out}} \times \sqrt{\texttt{fan_out}/\texttt{fan_in}} = \sqrt{3/\texttt{fan_in}}\).
but the scale is still no match for the sinusoids.
to fix that, the sinusoids are scaled down by \(\sqrt{d}\).
since the input embedding is essentially initialized with bound \(\sqrt{3/d}\),
this method consistently down-scales both the sinusoids and the input embedding by \(\sqrt{d}\),
and prevents extreme cases of aggressive scaling mentioned above.

stats of 2 models with =c4sm-samsam= architecture,
with vocabulary size \(8192\) and model dimension \(512\),
both having embedding sharing.
*old* uses the original initialization scheme, with \(\texttt{fan_avg}\) and up-scaled input embedding;
*new* uses \(\texttt{fan_out}\) and down-scaled sinusoids.

| scale            |    old |    new |
|------------------+--------+--------|
| sinusoid         | 1.0000 | 0.0442 |
| input embedding  | 0.5941 | 0.0765 |
| output embedding | 0.0263 | 0.0191 |
|------------------+--------+--------|
| bleu at 100k     |   32.9 |   33.0 |
| bleu at 200k     |   33.8 |   33.9 |

disabling embedding sharing made the accuracy slightly better and the loss slightly worse
but no difference to the bleu scores.

* attention

\begin{align*}
k , v , q &: \mathbb{N}_{+} &&\textrm{dimensions for key, value, query}\\
f_{k} &: \mathbb{R}^{v} \to \mathbb{R}^{k} &&\textrm{transformation for key}\\
f_{v} &: \mathbb{R}^{v} \to \mathbb{R}^{k} &&\textrm{transformation for value}\\
f_{q} &: \mathbb{R}^{q} \to \mathbb{R}^{k} &&\textrm{transformation for query}\\
\\
f_{a} &: \prod_{t : \mathbb{N}_{+}} \mathbb{R}^{t,k} \to \mathbb{R}^{k} \to \mathbb{R}^{t} &&\textrm{scaled dot-product attention}\\
f_{a} \; w \; x &= (w \; x) / \sqrt{k} &&\\
\\
f &: \prod_{t : \mathbb{N}_{+}} \mathbb{R}^{t,v} \to \mathbb{R}^{q} \to \mathbb{R}^{k} &&\textrm{the attention function}\\
f \; w \; x &= (f_{v} \; w)^{T} \; (f_{a} \; (f_{k} \; w) \; (f_{q} \; x)) &&\\
\end{align*}

* masking current step in self-attention

the current step almost always gets the highest attention weight,
but it is not necessary to consider itself since the residual connection will add itself back anyways.
[[https://arxiv.org/abs/1711.02281][gu et al. (2018)]] suggested masking current steps.
on the other hand, if the other steps offer no valuable info, current step should simply trust in itself.

empirically i found that the mask is useful.
the model does not learn as fast without the mask.

however when causal mask is present to enforce the autoregressive structure,
adding this mask means that the first and the last steps have nothing to attend to,
which results in nans.

consider these two alternatives.
- =padbos=: pad additional bos symbols to the input sequence,
  sacrifice (slice off) one bos at each self-attention layer when providing the attention queries.
- =padnil=: pad one initial step for the attention values, similar to causal convolution.

=padbos= is slightly messier to implement, however both alternatives are as efficient as the original =nomask=.
all three performs similarly, with =padnil= slightly better ([[https://github.com/ysmiraak/eti/tree/master/docs/stats/decoder-current-step-mask.acc.csv][stats]] per 250 updates).

* convolutional attention

[[https://arxiv.org/abs/1810.13320][yang et al.]] proposed convolutional self-attention.
the 1d version is basically the restricted attention mentioned by [[https://arxiv.org/abs/1706.03762][vaswani et al.]] in the original transformer paper.
the 2d version also queries across attention heads, which allows the averaging not to be simply component-wise.

* convolution with attention

stacking self-attention layers is wasteful.
all but the layers which connect to data show interesting patterns,
the others are mostly diagonal.
considering the cost of a attention layer is much higher than that of a convolutional layer,
we can replace all but one of them with convolution.

the encoder becomes a stack of bottleneck convolutional layers followed by the final attention layer.
this offers better performace on top of being more economical.

since the most expensive part with attention is in fact the mlp that follows.
maybe that can be replaced with convolution as well,
but so far the results are not as good.

how to improve the decoder with convolution?

* complexity

| b | 128 | bottleneck dimension |
| d | 512 | model dimension      |
| t |  64 | time steps           |

ignoring minor costs:
split, stack, scaling, masking, softmax, relu, residual connection, normalization, gain, and various biases;
which all have at most \(O(d)\) parameters and \(O(dt)\) complexity.

** original transformer self-attention layer

| parts      | parameters | complexity     |
|------------+------------+----------------|
| key        | d d        | d d t          |
| value      | d d        | d d t          |
| query      | d d        | d d t          |
| weight     |            | d t t          |
| average    |            | d t t          |
| mlp in     | d d 4      | d d t 4        |
| mlp ex     | d d 4      | d d t 4        |
|------------+------------+----------------|
| total      | 11 dd      | 11 ddt + 2 dtt |
| in million | 2.9        | 188.7          |

** bottleneck convolutional layer

| parts      | parameters  | complexity    |
|------------+-------------+---------------|
| in         | b d         | b d t         |
| conv1      | b b 2       | b b t 2       |
| conv2      | b b 2       | b b t 2       |
| ex         | b d         | b d t         |
|------------+-------------+---------------|
| total      | 2 bd + 4 bb | 2 bdt + 4 bbt |
| in million | 0.2         | 12.6          |
