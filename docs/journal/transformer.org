#+TITLE: transformer
#+AUTHOR: kuan yu

* preprocessing

[[https://github.com/google/sentencepiece][sentencepiece]] supports *bpe* ([[https://www.aclweb.org/anthology/P16-1162][sennrich et al.]]) and *unigram* ([[https://arxiv.org/abs/1804.10959][kudo]]) segmentations.
the latter supports non-deterministically sampled segmentations.

with the same vocabulary size, *unigram* produces shorter sequences than *bpe*, and performs slightly better.

training with sampled segmentations takes much longer to reach the same results.
eventually it should produce a better model, according to kudo.
however it does require tuning two hyperparameters, the smoothing rate and the sampling size.
for now i use only deterministic segmentations for training.

bleu scores with two layered transformer (=smsm-samsam= architecture).

| steps   | 100k | 200k |
|---------+------+------|
| sampled | 21.7 | 23.8 |
| unigram | 32.7 | 33.5 |
| bpe     | 32.8 | 33.0 |

* initialization

by default tensorflow uses *glorot* initialization ([[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf][glorot & bengio]]),
and it's common to use *he* initialization ([[https://arxiv.org/abs/1502.01852][he et al.]]) on layers with relu activation.
both are special cases of variance scaling.
with uniform initialization,
*glorot* sets the bound by \(\sqrt{3/n}\) and *he* by \(\sqrt{6/n}\),
where \(n\) is either \(\texttt{fan_in}\) (the input dimension),
\(\texttt{fan_out}\) (the output dimension),
or \(\texttt{fan_avg}\) (the average of the two).
the doubled scaling factor in *he* can be understood from the sparse nature of relu.

using \(\texttt{fan_in}\) preserves the variance during the forward propogation,
and \(\texttt{fan_out}\) preserves the variance during the backward propogation.
either one is sufficient condition for the model to converge.
\(\texttt{fan_avg}\) makes a trade-off between the two,
which i cannot make sense of.
usually *glorot* is used with \(\texttt{fan_avg}\) while *he* with \(\texttt{fan_in}\),
due to the suggestions of the authors.
however \(\texttt{fan_out}\) makes more sense.
consider a linear transformation \(\mathbb{R}^{p+q} \to \mathbb{R}^{r}\),
it can be factored into \(\mathbb{R}^{p} \to \mathbb{R}^{r}\) and \(\mathbb{R}^{q} \to \mathbb{R}^{r}\)
followed by vector addition.
only with \(\texttt{fan_out}\) that these two equivalent designed would be initialized similarly.

layer normalization makes initialization less important.
these variations result in little difference.
however input-output embedding sharing ([[https://arxiv.org/abs/1608.05859][press & wolf]]) and sinusoidal position encoding used in the transformer
introduces some complications.
since the sinusoidal encoding produces values in range \((-1,1)\),
the values in the input embedding cannot be too small.
however if they are on the same scale as the sinusoids,
the values in the ouput embedding would be too large for a logit layer,
since softmax activation will exponentiate those logits.
in the transformer, the shared matrix is initialized as usual with *glorot*,
but scaled up by \(\sqrt{d}\) when used as the input embedding,
where \(d\) is the model dimension.
this scaling seems reasonable by the same reasoning for scaled dot-product attention.
however when we consider how it interplays with the initialization scheme,
it is in fact an ad hoc remedy and can be too aggressive in extreme cases.
consider a character-level model with \(256\) characters and \(512\) model dimension,
the output embedding would be initialized with bound \(0.0884 \approx \sqrt{3 / ((512 + 256) / 2)}\),
which after up-scaling by \(\sqrt{512} \approx 22.63\)
results in an input embedding uniformly initialized with bound \(2.0\).

when the weights are initialized by \(\texttt{fan_out}\),
the scale of the input and output embeddings can be gracefully translated with
\(\sqrt{\texttt{fan_out}/\texttt{fan_in}}\)
since \(\sqrt{3/\texttt{fan_out}} \times \sqrt{\texttt{fan_out}/\texttt{fan_in}} = \sqrt{3/\texttt{fan_in}}\).
but the scale is still no match for the sinusoids.
to fix that, the sinusoids are scaled down by \(\sqrt{d}\).
since the input embedding is essentially initialized with bound \(\sqrt{3/d}\),
this method consistently down-scales both the sinusoids and the input embedding by \(\sqrt{d}\),
and prevents extreme cases of aggressive scaling mentioned above.

stats of 2 models with =c4sm-samsam= architecture,
with vocabulary size \(8192\) and model dimension \(512\),
both having embedding sharing.
*old* uses the original initialization scheme, with \(\texttt{fan_avg}\) and up-scaled input embedding;
*new* uses \(\texttt{fan_out}\) and down-scaled sinusoids.

| scale            |    old |    new |
|------------------+--------+--------|
| sinusoid         | 1.0000 | 0.0442 |
| input embedding  | 0.5941 | 0.0765 |
| output embedding | 0.0263 | 0.0191 |
|------------------+--------+--------|
| bleu at 100k     |   32.9 |   33.0 |
| bleu at 200k     |   33.8 |   33.9 |

disabling embedding sharing made the accuracy slightly better and the loss slightly worse
but no difference to the bleu scores.

* attention

\begin{align*}
k , v , q &: \mathbb{N}_{+} &&\textrm{dimensions for key, value, query}\\
f_{k} &: \mathbb{R}^{v} \to \mathbb{R}^{k} &&\textrm{transformation for key}\\
f_{v} &: \mathbb{R}^{v} \to \mathbb{R}^{k} &&\textrm{transformation for value}\\
f_{q} &: \mathbb{R}^{q} \to \mathbb{R}^{k} &&\textrm{transformation for query}\\
\\
f_{a} &: \prod_{t : \mathbb{N}_{+}} \mathbb{R}^{t,k} \to \mathbb{R}^{k} \to \mathbb{R}^{t} &&\textrm{scaled dot-product attention}\\
f_{a} \; w \; x &= (w \; x) / \sqrt{k} &&\\
\\
f &: \prod_{t : \mathbb{N}_{+}} \mathbb{R}^{t,v} \to \mathbb{R}^{q} \to \mathbb{R}^{k} &&\textrm{the attention function}\\
f \; w \; x &= (f_{v} \; w)^{T} \; (f_{a} \; (f_{k} \; w) \; (f_{q} \; x)) &&\\
\end{align*}

* masking current step in self-attention

the current step almost always gets the highest attention weight,
but it is not necessary to consider itself since the residual connection will add itself back anyways.
[[https://arxiv.org/abs/1711.02281][gu et al. (2018)]] suggested masking current steps.
on the other hand, if the other steps offer no valuable info, current step should simply trust in itself.

empirically i found that the mask is useful.
the model does not learn as fast without the mask.

however when causal mask is present to enforce the autoregressive structure,
adding this mask means that the first and the last steps have nothing to attend to,
which results in nans.

consider these two alternatives.
- =padbos=: pad additional bos symbols to the input sequence,
  sacrifice (slice off) one bos at each self-attention layer when providing the attention queries.
- =padnil=: pad one initial step for the attention values, similar to causal convolution.

=padbos= is slightly messier to implement, however both alternatives are as efficient as the original =nomask=.
all three performs similarly, with =padnil= slightly better ([[https://github.com/ysmiraak/eti/tree/master/docs/stats/decoder-current-step-mask.acc.csv][stats]] per 250 updates).

* affine vs linear

affine transformation is not necessary.

| =smsm-samsam= | 100k | 200k |
|---------------+------+------|
| affine        | 32.7 | 33.5 |
| linear        | 32.5 | 33.6 |

* convolutional attention

[[https://arxiv.org/abs/1810.13320][yang et al.]] proposed convolutional self-attention.
the 1d version is basically the restricted attention mentioned by [[https://arxiv.org/abs/1706.03762][vaswani et al.]] in the original transformer paper.
the 2d version also queries across attention heads, which allows the averaging not to be simply component-wise.

* convolution with attention

how much attention do you need?  [[https://aclweb.org/anthology/P18-1167][domhan 2018]]

stacking self-attention layers is wasteful.
all but the layers which connect to data show interesting patterns,
the others are mostly diagonal.
considering the cost of a attention layer is much higher than that of a convolutional layer,
we can replace some of them with convolution.

since the most expensive part with attention is in fact the mlp that follows.
hopefully that can be replaced with convolution as well.

** encoder

- in the encoder, conv blocks followed by self-attention works better than having just self-attention layers
- removing self-attention (fully convolutional like conv seq2seq) significantly reduced performance
- since conv is not the only force at work (self-attention), receptive field is not important,
  and complicated design (inception) does not help, only makes it more difficult to learn
  + filter banks (inception) don't help
  + filter size 3 and 2 performs the same
  + conv block 4 vs 6 performs the same
  + in each block, twice conv vs once performs the same
- conv dimensions around 128 is optimal, 64 works as well, 32 is too small, 256 too large
- non-linearity is more important than more convolution

| arch                  | 100k | 200k | conv block                         |    b |       k |
|-----------------------+------+------+------------------------------------+------+---------|
| smsm-samsam           | 32.5 | 33.6 |                                    |      |         |
| c6-samsam             | 31.4 | 32.3 | ante      conv relu conv relu post |  128 |       3 |
| c6sm-samsam           | 32.9 | 33.9 | ante      conv relu conv relu post |  128 |       2 |
| c4sm-samsam           | 32.9 | 33.9 | ante      conv relu conv relu post |  128 |       2 |
|-----------------------+------+------+------------------------------------+------+---------|
| c4sm-samsam_reluante  | 33.2 | 33.8 | ante relu conv relu conv relu post |  128 |       2 |
| c4sm-samsam_reluante2 | 32.9 | 33.9 | ante relu conv relu           post |  128 |       2 |
| c4sm-samsam_relu1     | 32.7 | 33.5 | ante      conv relu           post |  128 |       2 |
| c4sm-samsam_glu       | 32.9 | 33.9 | ante      conv glu            post |  128 |       2 |
| c4sm-samsam_grelu     | 32.9 |  n/a | ante relu conv glu            post |  128 |       2 |
| c1sm-samsam_glublock  | 33.0 | 33.8 | ante     (conv glu) x4        post |  128 |       2 |
| c2sm-samsam_glublock  | 33.0 | 34.1 | ante     (conv glu) x4        post |  128 |       2 |
| c2sm-samsam_block     | 32.9 | 33.9 | ante     (conv relu) x4       post |  128 |       2 |
|-----------------------+------+------+------------------------------------+------+---------|
| c4sm-samsam_bank      | 31.8 |  n/a | ante relu conv relu           post | 64*4 | 1 2 3 4 |
| c4sm-samsam_fs3       | 33.0 | 33.7 | ante relu conv relu           post |  128 |       3 |
| c4sm-samsam_fs3x2     | 32.9 | 33.9 | ante      conv relu conv relu post |  128 |       3 |
|-----------------------+------+------+------------------------------------+------+---------|
| c4sm-samsam_double    | 33.0 | 33.8 | ante      conv relu conv relu post |  256 |       2 |
| c4sm-samsam_half      | 32.9 | 33.9 | ante      conv relu conv relu post |   64 |       2 |
| c4sm-samsam_halfhalf  | 32.5 | 33.5 | ante      conv relu conv relu post |   32 |       2 |

** glu

- bottleneck 128 is the best
- stacking blocks is better than stacking layers
- bias not necessary even for sigmoid gate
- dropout only necessary for each block
- twin gates \(g \times y + (1-g) \times x\) don't work, make no changes to glu for now

| arch                       | 100k | 200k | k |   b |
|----------------------------+------+------+---+-----|
| csm-samsam                 | 33.0 | 33.8 | 4 | 128 |
| csm-samsam_half            | 33.0 | 33.6 | 4 |  64 |
| csm-samsam_double          | 33.1 | 33.8 | 4 | 256 |
| csm-samsam_depth2          | 32.9 | 33.7 | 2 | 128 |
| csm-samsam_depth3          | 32.8 | 33.9 | 3 | 128 |
| csm-samsam_depth4          | 32.9 | 33.9 | 4 | 128 |
| csm-samsam_depth5          | 32.9 | 33.8 | 5 | 128 |
| csm-samsam_depth6          | 33.0 | 33.8 | 6 | 128 |
| csm-samsam_depth8          | 33.0 | 33.7 | 8 | 128 |
| c3sm-samsam_depth2         | 33.0 | 34.2 | 2 | 128 |
| c2sm-samsam_depth4         | 33.1 | 34.0 | 4 | 128 |
| c2sm-samsam_depth4_nobias  | 33.2 | 34.0 | 4 | 128 |
| c2sm-samsam_depth4_twin    | 33.0 | 33.9 | 4 | 128 |
| c2sm-samsam_depth4_dropout | 33.0 | 33.9 | 4 | 128 |

- consider 512 to be 1 unit

| block             | params |
|-------------------+--------|
| mlp               |      8 |
| att               |      4 |
| conv relu depth 2 |    3/4 |
| conv glu depth 2  |      1 |
| conv glu depth 4  |    1.5 |

** decoder

| arch                    | 100k | 200k |
|-------------------------+------+------|
| smsm-samsam             | 32.6 | 33.7 |
| c3sc3sc3-c3bc3bc3       | 33.0 | 33.8 |
| c3sc3sc3-c3sac3sac3     | 33.3 | 34.1 |
| c3sc3sc3-c3asc3asc3     | 32.9 | 34.1 |
| c3sc3sc3-cscacscacsc    | 33.2 | 34.1 |
| c3sc3sc3-c3asc3ac3sac3  | 33.4 | 34.1 |
| c3sc3sc3-samsam         | 33.1 | 34.3 |
| c3sc3sc3-c3sac3sc3asc3  | 33.4 | 34.3 |
| c3sc3sc3-c3ac3sc3ac3sc3 | 33.4 | 34.5 |
| c3sc3sc3-c3sc3ac3sc3ac3 | 33.3 | 34.6 |

the 0.1 improvement for =smsm-samsam= in comparison to previous results
was due to the addition of an output transformation in attention blocks,
which was stated in the original transformer paper but i missed it.

* complexity

| b | 128 | bottleneck dimension |
| d | 512 | model dimension      |
| t |  64 | time steps           |

ignoring minor costs:
split, stack, scaling, masking, sigmoid, softmax, relu, residual connection, normalization, gain, and bias;
which all have at most \(O(d)\) parameters and \(O(dt)\) complexity.

** mlp layer

| parts      | parameters | complexity |
|------------+------------+------------|
| in         | d d 4      | d d t 4    |
| ex         | d d 4      | d d t 4    |
|------------+------------+------------|
| total      | 8 dd       | 8 ddt      |
| in million | 2.10       | 134.22     |

** attention layer

| parts      | parameters | complexity    |
|------------+------------+---------------|
| key        | d d        | d d t         |
| value      | d d        | d d t         |
| query      | d d        | d d t         |
| output     | d d        | d d t         |
| weight     |            | d t t         |
| average    |            | d t t         |
|------------+------------+---------------|
| total      | 4 dd       | 4 ddt + 2 dtt |
| in million | 1.05       | 71.30         |

** bottleneck glu layer

| parts      | parameters  | complexity    |
|------------+-------------+---------------|
| in         | b d         | b d t         |
| conv1      | b b 2       | b b t 2       |
| gate1      | b b 2       | b b t 2       |
| conv2      | b b 2       | b b t 2       |
| gate2      | b b 2       | b b t 2       |
| ex         | b d         | b d t         |
|------------+-------------+---------------|
| total      | 2 bd + 8 bb | 2 bdt + 8 bbt |
| in million | 0.26        | 16.78         |
