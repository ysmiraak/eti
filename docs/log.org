* trial

- a encoder-decoder, autoregressive (autoreg)
- s straight seq2seq, weighted loss (straight)
- n encoder-bridge-decoder, nonautoregressive, weighted loss (master)
- o encoder-bridge-decoder, nonautoregressive, nonweighted loss (master)
- p encoder-decoder, nonautoregressive, weighted loss (position)

* findings

** masking current step

thought: if the other steps offer no valuable info, current step
should simply trust in itself.

found: the mask is useful.  the model does not learn as fast without
the mask

* todo

investigate the cause for indecisiveness of middle steps

** try use emb_pos as position encoding

probably doesn't make sense
but it may help coordinating tgt and src

** try without dropout on emb_pos

when and where should we apply dropout?

** try convolution with self-attention
