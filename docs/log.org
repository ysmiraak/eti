* trial

** old

| id | branch    | info                                                 |
|----+-----------+------------------------------------------------------|
| a  | autoreg   | encoder-decoder, autoregressive                      |
| s  | straight  | straight seq2seq                                     |
| v  | master    | encoder-decoder, nonautoregressive                   |
|    |           |                                                      |
| i  | iterative | iterated decoder                                     |
|    |           |                                                      |
| d  | droptgt   | like v but with target input with increasing dropout |

** autoregressive

| arch           | id  |    1 |    2 | note                                        | branch                 |
|----------------+-----+------+------+---------------------------------------------+------------------------|
| smsm-samsam    | a   | 21.7 | 23.8 | original transformer with current step mask | autoreg                |
| smsm-smamsm    | p   | 20.9 | 22.9 | mlp after each attention                    | postnet                |
| smsm-samsam_he | he  | 19.8 | 21.2 | same sized relu layers with he init         | autoreg-init           |
| c6sm-samsam    | c   | 22.2 | 24.1 |                                             | autoreg-conv           |
| c4sm-samsam    | cs  | 22.6 | 24.0 |                                             | autoreg-conv           |
| c4sc2-samsam   | cc  | 21.9 | 23.1 | can conv replace mlp ????                   | autoreg-conv           |
| csccsc-samsam  | csc | 21.4 | 23.1 | conv-satt-conv encoder layers               | autoreg-conv-satt-conv |
| c4sm-c4sam     | cd  | 20.9 | 22.0 |                                             | autoreg-conv-dec       |
| c4sm-c4sasm    | cdd | 21.1 | 22.6 |                                             | autoreg-cdd            |
| c4sm-c4sac4sm  | ccd | 20.8 |      |                                             | autoreg-ccd            |

* todo

** convolutional

** residual vs highway vs bilinear

** attention mechanism

- affine for key, value, query transformations
- query mlp

** input output embedding sharing

** dropout position encoding as well?
