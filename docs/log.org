* trial

** old

| id | branch    | info                                                 |
|----+-----------+------------------------------------------------------|
| a  | autoreg   | encoder-decoder, autoregressive                      |
| s  | straight  | straight seq2seq                                     |
| v  | master    | encoder-decoder, nonautoregressive                   |
|    |           |                                                      |
| i  | iterative | iterated decoder                                     |
|    |           |                                                      |
| d  | droptgt   | like v but with target input with increasing dropout |

** autoregressive

| id | branch       | info                                                                       |
|----+--------------+----------------------------------------------------------------------------|
| p  | postnet      | (mlp . csl . mlp . att . mlp . csl) <- (mlp . att . csl . mlp . att . csl) |
| pc | postnet-conv | postnet with convolutional encoder                                         |
|    |              |                                                                            |
| a  | autoreg      | original transformer with current step mask                                |
| he | autoreg-init | he init for relu without large middle layers                               |

* todo

** are quadrupled middle layers in mlps necessary

bigger middle layers prevent dying relu,
but maybe doubled size is enough.
or change to he initialization?

** convolutional

** attention mechanism

- affine for key, value, query transformations
- query mlp

** input output embedding sharing

** dropout position encoding as well?
