* trial

- a encoder-decoder, autoregressive (autoreg)
- s straight seq2seq (straight)
- v encoder-bridge-decoder, nonautoregressive (master)
- p encoder-decoder, nonautoregressive (position)

* findings

** masking current step

thought: if the other steps offer no valuable info, current step
should simply trust in itself.

found: the mask is useful.  the model does not learn as fast without
the mask

** weighted loss over seq

apply linearly increasing weights on the loss over sequence 1,2,3,...,t,1,...,1

does not seem to work and even hinders learning

maybe soften the weights by sqrt?

* todo

investigate the cause for indecisiveness of middle steps

** try use emb_pos as position encoding

probably doesn't make sense
but it may help coordinating tgt and src

** try without dropout on emb_pos

when and where should we apply dropout?

** try convolution with self-attention
