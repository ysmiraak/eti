* trial

| id | branch                  | info                                                 |
|----+-------------------------+------------------------------------------------------|
| a  | autoreg                 | encoder-decoder, autoregressive                      |
| s  | straight                | straight seq2seq                                     |
| v  | master                  | encoder-decoder, nonautoregressive                   |
|    |                         |                                                      |
| i  | variational             | iterated decoder, nonautoregressive                  |
| j  | variational-for-real    | iterated decoder from embedding to logit             |
|    |                         |                                                      |
| l  | postion-weighted-linear | like v but with linear weight over loss              |
| r  | postion-weighted-sqrt   | like v but with sqrt weight over loss                |
|    |                         |                                                      |
| d  | droptgt                 | like v but with target input with increasing dropout |

* findings

** masking current step

thought:

if the other steps offer no valuable info,
current step should simply trust in itself.

found:

the mask is useful.
the model does not learn as fast without the mask.

however when causal mask is present,
this means that the first and last step in self-attention
have no values to attend to,
which results in nans

** weighted loss over seq

apply linearly increasing weights on the loss over sequence
1,2,3,...,t,1,...,1

does not seem to work and even hinders learning

maybe soften the weights by sqrt?

* todo

** try use emb_pos as position encoding

probably doesn't make sense
but it may help coordinating tgt and src

** try without dropout on emb_pos

when and where should we apply dropout?

** try convolution with self-attention
