* trial

- a encoder-decoder, autoregressive (autoreg)
- s straight seq2seq (straight)
- v encoder-bridge-decoder, nonautoregressive (master)
- p encoder-decoder, nonautoregressive (position)
- q like p but with decoder causal mask (position-causal)

- i iterated decoder, nonautoregressive (variational)
- l like p but with linear weight over loss (postion-weighted-linear)
- r like p but with sqrt weight over loss (postion-weighted-sqrt)

* findings

** masking current step

thought:

if the other steps offer no valuable info,
current step should simply trust in itself.

found:

the mask is useful.
the model does not learn as fast without the mask.

however when causal mask is present,
this means that the first and last step in self-attention
have no values to attend to,
which results in nans

** weighted loss over seq

apply linearly increasing weights on the loss over sequence
1,2,3,...,t,1,...,1

does not seem to work and even hinders learning

maybe soften the weights by sqrt?

* todo

investigate the cause for indecisiveness of middle steps

** try use emb_pos as position encoding

probably doesn't make sense
but it may help coordinating tgt and src

** try without dropout on emb_pos

when and where should we apply dropout?

** try convolution with self-attention
