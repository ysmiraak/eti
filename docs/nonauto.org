non-autoregressive model

- teacher-forcing not robust
- scheduled sampling is slow
- beam-search is a hack

* van den oord et al. 2017

https://arxiv.org/abs/1711.10433

- parallel wavenet
- probability density distillation
- inverse autoregressive flows

* gu el al. 2017, chen et al. 2018

https://arxiv.org/abs/1702.02429
https://arxiv.org/abs/1804.07915

- trainable greedy decoding
- actor network

* tang et al. 2018

https://arxiv.org/abs/1710.10380

- non-autoregressive convolutional decoding
- sentence representation learning

experiments

- autoregressive rnn and cnn decoder with
  + teacher-forcing
  + multinomial sampling
  + uniform sampling
- non-autoregressive aka predict-all-words cnn

findings

- it is not necessary to input the correct words into an
  autoregressive decoder for learning sentence representations
- the model with an autoregressive decoder performs similarly to the
  model with a predict-all-words decoder

* gu et al. 2018

https://arxiv.org/abs/1711.02281

- non-autoregressive
- fertility feature
- self-attention current position mask

* wang el al. 2018

https://arxiv.org/abs/1808.08583

- semi-autoregressive aka batched autoregressive
- relatex causal mask

* schmidt & hofmann 2018

https://arxiv.org/abs/1806.04550

- unconditional word generation
- state space model
- variational inference

* lee et al. 2018

https://arxiv.org/abs/1802.06901

- deterministic non-autoregressive sequence modeling
- iterative refinement

* roy et al. 2018

https://arxiv.org/abs/1805.11063

- vector quentized vae

* kaiser et al. 2018

https://arxiv.org/abs/1803.03382

- autoregressive discrete latent variables
- parallel decoding from latent sequence
- decomposed vector quantization
